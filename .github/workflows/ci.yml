# file: .github/workflows/rag-ci.yml
name: RAG CI

on:
  push:
  pull_request:

jobs:
  eval:
    runs-on: ubuntu-latest
    env:
      PYTHONUNBUFFERED: "1"
      CHAT_MODEL: "mock"
      EMBED_MODEL: "mock"
      DB_PATH: "./chroma_db"
      DOCS_PATH: "./materiales"
      COLLECTION: "capacitacion"
      HF_HOME: ~/.cache/huggingface
      TRANSFORMERS_CACHE: ~/.cache/huggingface

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # ---- Python con cache de pip usando requirements.txt ----
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: |
            requirements.txt
            requirements-dev.txt

      - name: Install Python deps (requirements.txt)
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt
          # opcional (si tienes extras de dev):
          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi

      # ---- Cache de HuggingFace (para sentence-transformers) ----
      - name: Cache HuggingFace models
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/huggingface
          key: hf-${{ runner.os }}-${{ hashFiles('requirements.txt','requirements-dev.txt') }}
          restore-keys: |
            hf-${{ runner.os }}-

      # ---- Node + cache NPM global (promptfoo) ----
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Cache npm global
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: npm-${{ runner.os }}-node20-promptfoo
          restore-keys: |
            npm-${{ runner.os }}-node20-

      - name: Install promptfoo
        run: npm i -g promptfoo

      # --- MOCKS para CI (sin Ollama real) ---
      - name: Start fake Ollama (port 11434)
        run: |
          python - <<'PY' &
          from fastapi import FastAPI; import uvicorn
          app = FastAPI()
          @app.get("/api/tags")
          def tags(): return {"models":[{"name":"mock"}]}
          uvicorn.run(app, host="127.0.0.1", port=11434, log_level="warning")
          PY
          echo $! > fake_ollama.pid

      - name: Generate training PDF
        run: python generate_training_pdf.py

      - name: Build index (using mocked ollama embeddings)
        env:
          PYTHONPATH: "tools/ci_mocks"
        run: python build_index.py

      - name: Start API server
        env:
          PYTHONPATH: "tools/ci_mocks"
        run: |
          nohup uvicorn server:app --host 0.0.0.0 --port 8000 > server.log 2>&1 &
          echo $! > server.pid

      - name: Wait for /health
        run: |
          for i in {1..60}; do
            code=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8000/health || true)
            if [ "$code" = "200" ]; then echo "OK /health"; exit 0; fi
            sleep 2
          done
          echo "Timeout esperando /health"; cat server.log; exit 1

      - name: Ingest KPIs snippet (opcional)
        run: |
          curl -s -X POST http://localhost:8000/ingest_text \
            -H "Content-Type: application/json" \
            -d '{"source_name":"kpis.md","text":"SecciÃ³n de KPIs:\nCSAT, NPS, FCR, AHT, SLA con metas y descripciones."}' | tee ingest_kpis.json

      - name: Run promptfoo eval
        run: |
          test -f promptfooconfig.yaml || (echo "faltante: promptfooconfig.yaml"; exit 1)
          test -f tests.yaml || (echo "faltante: tests.yaml"; exit 1)
          promptfoo eval -c promptfooconfig.yaml -c tests.yaml || (echo "::error ::Promptfoo failures"; exit 1)

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: rag-ci-artifacts
          path: |
            server.log
            ingest_kpis.json
            .promptfoo/**

      - name: Cleanup
        if: always()
        run: |
          kill $(cat server.pid) || true
          kill $(cat fake_ollama.pid) || true

      # --- Reportes extra (opcional) ---
      - name: Run promptfoo eval (JSON + Markdown + JUnit)
        run: |
          export PROMPTFOO_TELEMETRY=0
          promptfoo eval -c promptfooconfig.yaml -c tests.yaml --format json     --output .promptfoo/results.json
          promptfoo eval -c promptfooconfig.yaml -c tests.yaml --format markdown --output .promptfoo/report.md
          promptfoo eval -c promptfooconfig.yaml -c tests.yaml --format junit    --output .promptfoo/promptfoo.junit.xml

      - name: Publish Promptfoo summary
        if: always()
        run: |
          echo "## Promptfoo Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          test -f .promptfoo/report.md && cat .promptfoo/report.md >> $GITHUB_STEP_SUMMARY || echo "_no markdown report_"

      - name: Upload Promptfoo artifacts (reports)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: promptfoo-reports
          path: |
            .promptfoo/results.json
            .promptfoo/report.md
            .promptfoo/promptfoo.junit.xml
